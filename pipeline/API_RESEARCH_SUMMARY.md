# API Research Summary - MÃ¼nchen/Isar Pegeldaten

**Datum:** 25. Januar 2026  
**Station:** MÃ¼nchen / Isar (ID: 16005701)  
**Ziel:** Automatisiertes Abrufen aktueller Pegeldaten (alle 3 Stunden)

---

## ğŸ“Š Ergebnisse der Recherche

### âœ… Was funktioniert: HTML-Scraping

**LÃ¶sung:** Web-Scraping von hnd.bayern.de

- **URL:** https://www.hnd.bayern.de/pegel/isar/muenchen-16005701/tabelle?methode=wasserstand&setdiskr=15
- **Datenformat:** HTML-Tabelle mit 15-Minuten-Werten
- **Historische Daten:** ~7 Tage (641 Datenpunkte Ã  15 Minuten)
- **ZuverlÃ¤ssigkeit:** Stabil, gut scrapbar
- **Update-Frequenz:** Alle 15 Minuten

### âŒ Was NICHT verfÃ¼gbar ist:

#### 1. PEGELONLINE REST-API
- **Status:** âŒ MÃ¼nchen/Isar nicht verfÃ¼gbar
- **Grund:** PEGELONLINE deckt nur BundeswasserstraÃŸen ab
- **Isar MÃ¼nchen:** Unter bayerischer Landeshoheit (LfU Bayern)
- **Getestet:** âœ… (siehe `test_pegelonline.py`)

#### 2. Offizielle JSON/REST-APIs
Getestete Endpoints (alle 404):
- âŒ `hnd.bayern.de/webservices/daten.php`
- âŒ `hnd.bayern.de/webservices/messwerte.php`
- âŒ `hnd.bayern.de/api/pegel/{id}`
- âŒ `gkd.bayern.de/api/fluesse/wasserstand/{id}`
- âœ… `hnd.bayern.de/webservices/graphik.php` (nur PNG-Grafik, keine Daten)
- **Getestet:** âœ… (siehe `test_hnd_endpoints.py`)

#### 3. RSS/Atom Feeds
Getestete Feed-URLs (alle 404):
- âŒ `hnd.bayern.de/rss/pegel/{id}`
- âŒ `hnd.bayern.de/warnungen/rss`
- âŒ Keine Feed-Links in HTML-Meta-Tags gefunden
- **Getestet:** âœ… (siehe `test_rss_feeds.py`)

---

## ğŸ› ï¸ Implementierte LÃ¶sung

### Scripts

1. **`fetch_isar_current.py`**
   - Holt alle verfÃ¼gbaren Messwerte (641 Datenpunkte)
   - Speichert in `current_water_level.json`
   - FÃ¼r manuelle Analyse/Debugging

2. **`fetch_and_store_isar.py`** â­ HAUPTSCRIPT
   - Holt nur den neuesten Wert
   - Speichert in JSONL-Format (tÃ¤gliche Dateien)
   - Vermeidet Duplikate
   - Designed fÃ¼r Cron-Job (alle 3 Stunden)
   - Umfangreiches Logging

### Datenformat

**JSONL (JSON Lines):** Eine Messung pro Zeile

```json
{
  "timestamp": "2026-01-25T16:00:00",
  "timestamp_unix": 1769353200,
  "date": "2026-01-25",
  "time": "16:00:00",
  "value_cm": 87,
  "unit": "cm",
  "station_id": "16005701",
  "station_name": "MÃ¼nchen / Isar",
  "source": "hnd.bayern.de",
  "fetched_at": "2026-01-25T16:18:19.230200"
}
```

### Automatisierung

**Cron-Job Beispiel:**
```cron
# Alle 3 Stunden um :05 nach
5 */3 * * * cd /home/retroflex/monoroc/isarwasser/pipeline && /usr/bin/python3 fetch_and_store_isar.py >> /home/retroflex/monoroc/isarwasser/log.txt 2>&1
```

**Zeitpunkte:** 00:05, 03:05, 06:05, 09:05, 12:05, 15:05, 18:05, 21:05

---

## ğŸ“ˆ Vergleich der Optionen

| Option | VerfÃ¼gbarkeit | ZuverlÃ¤ssigkeit | Aufwand | Empfehlung |
|--------|--------------|-----------------|---------|------------|
| PEGELONLINE API | âŒ Nicht verfÃ¼gbar | N/A | N/A | âŒ |
| Offizielle API | âŒ Existiert nicht | N/A | N/A | âŒ |
| RSS Feeds | âŒ Nicht verfÃ¼gbar | N/A | N/A | âŒ |
| HTML Scraping | âœ… VerfÃ¼gbar | â­â­â­â­ Gut | â­â­ Mittel | âœ… **BESTE OPTION** |
| LfU kontaktieren | ğŸ¤” MÃ¶glich | â­â­â­ ? | â­â­â­â­ Hoch | ğŸ’¡ Langfristig |

---

## ğŸ¯ Empfehlungen

### Kurzfristig (JETZT):
1. âœ… **HTML-Scraping nutzen** (implementiert und funktioniert)
2. âœ… Cron-Job einrichten (siehe `CURRENT_DATA_README.md`)
3. âœ… Mit historischen Daten kombinieren

### Mittelfristig:
1. ğŸ“Š Daten in DuckDB integrieren
2. ğŸŒ Live-Daten in Web-App visualisieren
3. ğŸ“ˆ Monitoring fÃ¼r Scraping-Fehler einrichten

### Langfristig:
1. ğŸ“§ **LfU Bayern kontaktieren** (hnd@lfu.bayern.de)
   - Nach offizieller API fragen
   - Projekt vorstellen
   - Auf Bildungszweck hinweisen
2. ğŸ¤ Falls API verfÃ¼gbar wird: Migration von Scraping zu API

---

## âš–ï¸ Rechtliche Ãœberlegungen

### âœ… FÃ¼r Scraping:
- Daten sind Ã¶ffentlich zugÃ¤nglich (keine Paywall)
- Angemessenes Rate-Limiting (alle 3 Stunden)
- Identifiziert als "educational project"
- Keine kommerziellen Zwecke
- Respektiert robots.txt (falls vorhanden)

### âš ï¸ Risiken:
- Website-Struktur kÃ¶nnte sich Ã¤ndern (â†’ Scraper muss angepasst werden)
- LfU kÃ¶nnte Scraping explizit untersagen (â†’ dann API anfragen)
- Server-Last durch zu hÃ¤ufige Requests (â†’ durch 3h-Intervall vermieden)

---

## ğŸ“š Dateien

- âœ… `test_pegelonline.py` - Testet PEGELONLINE API
- âœ… `test_hnd_endpoints.py` - Testet versteckte APIs
- âœ… `test_rss_feeds.py` - Testet RSS/Atom Feeds
- âœ… `fetch_isar_current.py` - Holt alle Werte (Debug)
- âœ… `fetch_and_store_isar.py` - Production Script fÃ¼r Cron
- âœ… `CURRENT_DATA_README.md` - Setup-Anleitung
- âœ… `API_RESEARCH_SUMMARY.md` - Dieses Dokument
- âœ… `requirements.txt` - Updated mit beautifulsoup4

---

## ğŸ Status

**âœ… ABGESCHLOSSEN**

Die Recherche ist vollstÃ¤ndig. Die einzige praktikable LÃ¶sung ist HTML-Scraping, und diese ist implementiert und getestet.

**NÃ¤chster Schritt:** Cron-Job einrichten und Datensammlung starten! ğŸš€
